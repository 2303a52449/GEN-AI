{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSUq//98ortlt1cCur48wq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52449/GEN-AI/blob/main/GENAI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "YActual YP red\n",
        "20 20.5\n",
        "30 30.3\n",
        "40 40.2\n",
        "50 50.6\n",
        "60 60.7\n",
        "Tabela 1: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "vWPus_uuMWCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re91mwChL_In",
        "outputId": "ab6762bc-f01a-4e8b-f566-026afdbea6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Values:  [20, 30, 40, 50, 60]\n",
            "Predicted Values:  [20.5, 30.3, 40.2, 50.6, 60.7]\n",
            "Errors (Y_actual - Y_pred):  [-0.5, -0.3000000000000007, -0.20000000000000284, -0.6000000000000014, -0.7000000000000028]\n",
            "Mean Absolute Error (MAE):  0.4600000000000016\n",
            "Mean Squared Error (MSE):  0.24600000000000147\n",
            "Root Mean Squared Error (RMSE):  0.49598387070549127\n",
            "\n",
            "Comparison of Actual vs Predicted:\n",
            "Actual: 20, Predicted: 20.5, Error: -0.5\n",
            "Actual: 30, Predicted: 30.3, Error: -0.3000000000000007\n",
            "Actual: 40, Predicted: 40.2, Error: -0.20000000000000284\n",
            "Actual: 50, Predicted: 50.6, Error: -0.6000000000000014\n",
            "Actual: 60, Predicted: 60.7, Error: -0.7000000000000028\n"
          ]
        }
      ],
      "source": [
        "# Define actual and predicted values\n",
        "Y_actual = [20, 30, 40, 50, 60]\n",
        "Y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n",
        "\n",
        "# Calculate errors\n",
        "errors = [actual - pred for actual, pred in zip(Y_actual, Y_pred)]\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = sum(abs(error) for error in errors) / len(errors)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = sum(error ** 2 for error in errors) / len(errors)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "# Print the results\n",
        "print(\"Actual Values: \", Y_actual)\n",
        "print(\"Predicted Values: \", Y_pred)\n",
        "print(\"Errors (Y_actual - Y_pred): \", errors)\n",
        "print(\"Mean Absolute Error (MAE): \", mae)\n",
        "print(\"Mean Squared Error (MSE): \", mse)\n",
        "print(\"Root Mean Squared Error (RMSE): \", rmse)\n",
        "\n",
        "# Comparison of results\n",
        "print(\"\\nComparison of Actual vs Predicted:\")\n",
        "for actual, pred, error in zip(Y_actual, Y_pred, errors):\n",
        "    print(f\"Actual: {actual}, Predicted: {pred}, Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red\n"
      ],
      "metadata": {
        "id": "c80kNN6qMc99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    unique_classes = np.unique(y_true)\n",
        "    TP, FP, FN = {}, {}, {}\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        TP[cls] = sum((y_true == cls) & (y_pred == cls))\n",
        "        FP[cls] = sum((y_true != cls) & (y_pred == cls))\n",
        "        FN[cls] = sum((y_true == cls) & (y_pred != cls))\n",
        "\n",
        "    precision = {cls: TP[cls] / (TP[cls] + FP[cls]) if (TP[cls] + FP[cls]) > 0 else 0 for cls in unique_classes}\n",
        "    recall = {cls: TP[cls] / (TP[cls] + FN[cls]) if (TP[cls] + FN[cls]) > 0 else 0 for cls in unique_classes}\n",
        "    f1 = {cls: 2 * (precision[cls] * recall[cls]) / (precision[cls] + recall[cls]) if (precision[cls] + recall[cls]) > 0 else 0 for cls in unique_classes}\n",
        "    accuracy = sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Given values\n",
        "y_true = np.array([0, 0, 0, 0, 0, 1, 2, 2, 2, 2])\n",
        "y_pred = np.array([0, 1, 1, 1, 1, 2, 0, 0, 2, 2])\n",
        "\n",
        "# Manual computation\n",
        "accuracy, precision, recall, f1 = calculate_metrics(y_true, y_pred)\n",
        "\n",
        "print(\"Manual Calculation:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Using sklearn\n",
        "print(\"\\nUsing sklearn:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_true, y_pred, average=None)}\")\n",
        "print(f\"Recall: {recall_score(y_true, y_pred, average=None)}\")\n",
        "print(f\"F1 Score: {f1_score(y_true, y_pred, average=None)}\")\n"
      ],
      "metadata": {
        "id": "a_kZk9LJMjzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d010e2-7e21-47f2-ffe1-376683e10e4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculation:\n",
            "Accuracy: 0.30\n",
            "Precision: {0: 0.3333333333333333, 1: 0.0, 2: 0.6666666666666666}\n",
            "Recall: {0: 0.2, 1: 0.0, 2: 0.5}\n",
            "F1 Score: {0: 0.25, 1: 0, 2: 0.5714285714285715}\n",
            "\n",
            "Using sklearn:\n",
            "Accuracy: 0.30\n",
            "Precision: [0.33333333 0.         0.66666667]\n",
            "Recall: [0.2 0.  0.5]\n",
            "F1 Score: [0.25       0.         0.57142857]\n"
          ]
        }
      ]
    }
  ]
}